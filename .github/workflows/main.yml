name: Download and Quantize Hugging Face Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: 'Hugging Face Model Repository ID (e.g., gpt2, bert-base-uncased)'
        required: true
        default: 'gpt2'  # 设置一个默认值

jobs:
  download_and_quantize:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    - name: Install dependencies
      run: |
        pip install transformers huggingface_hub bitsandbytes

    - name: Authenticate to Hugging Face
      run: |
        echo "HUGGINGFACE_TOKEN=${{ secrets.HF_TOKEN }}" >> $GITHUB_ENV

    - name: Download the model from Hugging Face
      run: |
        from huggingface_hub import login
        login(token="${{ secrets.HF_TOKEN }}")
        from transformers import AutoModelForCausalLM, AutoTokenizer

        # Get the model name from the input
        model_name = "${{ github.event.inputs.model_name }}"  # 从输入获取模型仓库 ID
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Save the model and tokenizer (optional)
        model.save_pretrained('./model')
        tokenizer.save_pretrained('./model')

    - name: Quantize the model to Q4_K_M
      run: |
        from transformers import AutoModelForCausalLM
        import bitsandbytes as bnb
        
        # Load the model
        model_name = './model'  # path to the model directory
        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb.optim.GlobalQuantConfig(4))

        # Save the quantized model
        model.save_pretrained('./quantized_model')

    - name: Upload Quantized Model
      uses: actions/upload-artifact@v2
      with:
        name: quantized-model
        path: ./quantized_model
