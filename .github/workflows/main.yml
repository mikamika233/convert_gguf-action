name: Convert HF Model to GGUF and Quantize (Multi-Precision)

on:
  workflow_dispatch:
    inputs:
      repo_id:
        description: 'Hugging Face model repo ID (e.g., TheBloke/Mistral-7B-Instruct-v0.1)'
        required: true
      branch:
        description: 'Branch name (default: main)'
        required: false
        default: 'main'
      quantization_levels:
        description: 'Comma-separated list of quantization types (e.g., Q4_K_M,Q5_K_M,Q8_0)'
        required: false
        default: 'Q4_K_M'

jobs:
  convert:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Python Dependencies
      run: |
        pip install -U huggingface_hub transformers sentencepiece

    - name: Authenticate HF
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "HUGGINGFACE_TOKEN=${HF_TOKEN}" >> $GITHUB_ENV

    - name: Download model from Hugging Face
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        python3 -c "from huggingface_hub import snapshot_download; snapshot_download(repo_id='${{ github.event.inputs.repo_id }}', revision='${{ github.event.inputs.branch }}', token='${{ secrets.HF_TOKEN }}', local_dir='hf-model', local_dir_use_symlinks=False)"

    - name: Download latest llama.cpp binaries
      run: |
        mkdir -p llama-bin
        LATEST_URL=$(curl -s https://api.github.com/repos/ggml-org/llama.cpp/releases/latest \
          | grep "llama-.*-ubuntu-x64.zip" \
          | grep "browser_download_url" \
          | cut -d '"' -f 4)
        echo "Downloading from: $LATEST_URL"
        wget -q "$LATEST_URL" -O llama-bin/llama-bin.zip
        unzip -q llama-bin/llama-bin.zip -d llama-bin
        chmod +x llama-bin/*


    - name: Clone llama.cpp
      run: |
        git clone https://github.com/ggml-org/llama.cpp.git
        cd llama.cpp
        pip install -r requirements.txt

    - name: Convert to GGUF
      run: |
        cd llama.cpp
        mkdir -p ../converted
        python3 convert_hf_to_gguf.py ../hf-model --outfile ../converted/model.gguf

    - name: Quantize to Multiple Levels
      run: |
        mkdir -p quantized
        cd llama-bin/build/bin/
        IFS=',' read -ra LEVELS <<< "${{ github.event.inputs.quantization_levels }}"
        for level in "${LEVELS[@]}"; do
          ./llama-quantize ../../../converted/model.gguf ../../../quantized/model.${level}.gguf $level 4
        done



    - name: Upload Quantized Models
      uses: actions/upload-artifact@v4
      with:
        name: quantized-gguf-models
        path: quantized/*.gguf
